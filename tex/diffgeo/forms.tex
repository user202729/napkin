\chapter{Differential forms}
In this chapter, all vector spaces are finite-dimensional
real inner product spaces.
We first start by (non-rigorously) drawing pictures
of all the things that we will define in this chapter.
Then we re-do everything again in its proper algebraic context.

\section{Pictures of differential forms}
Before defining a differential form,
we first draw some pictures.
The key thing to keep in mind is
\begin{moral}
	``The definition of a differential form is:
	something you can integrate.'' \\ --- Joe Harris
\end{moral}

We'll assume that all functions are \vocab{smooth},
i.e.\ infinitely differentiable.

Let $U \subseteq V$ be an open set of a vector space $V$.
Suppose that we have a function $f \colon U \to \RR$, i.e.\
we assign a value to every point of $U$.
\begin{center}
	\begin{asy}
		bigblob("$U$");
		dot("$3$", (-2,1), red);
		dot("$\sqrt2$", (-1,-1), red);
		dot("$-1$", (2,2), red);
		dot("$0$", (-3,-3), red);
	\end{asy}
\end{center}
\begin{definition}
	A \vocab{$0$-form} $f$ on $U$ is just a smooth function $f \colon U \to \RR$.
\end{definition}
Thus, if we specify a finite set $S$ of points in $U$
we can ``integrate'' over $S$ by just adding up the values
of the points:
\[ 0 + \sqrt 2 + 3 + (-1) = 2 + \sqrt2. \]
So, \textbf{a $0$-form $f$ lets us integrate over $0$-dimensional ``cells''}.

But this is quite boring, because as we know we like
to integrate over things like curves, not single points.
So, by analogy, we want a $1$-form to let us integrate
over $1$-dimensional cells: i.e.\ over curves.
What information would we need to do that?
To answer this, let's draw a picture of a curve $c$,
which can be thought of as a function $c \colon [0,1] \to U$.
\begin{center}
	\begin{asy}
		bigblob("$U$");
		pair a = (-2,-2);
		pair b = (3,0);
		pair p = (0,1);
		pair q = (2,0);
		label("$c$", q, dir(45), heavygreen);
		dot(a, heavygreen);
		dot(b, heavygreen);
		draw(a..p..q..b, heavygreen);
		dot("$p$", p, dir(90), blue);
		pair v = p+1.5*dir(-10);
		label("$v$", v, dir(50), blue);
		draw(p--v, blue, EndArrow);
	\end{asy}
\end{center}
We might think that we could get away
with just specifying a number on every point of $U$
(i.e.\ a $0$-form $f$), and then somehow ``add up''
all the values of $f$ along the curve.
We'll use this idea in a moment, but we can in fact do something more general.
Notice how when we walk along a smooth curve, at every point $p$
we also have some extra information: a \emph{tangent vector} $v$.
So, we can define a $1$-form $\alpha$ as follows.
A $0$-form just took a point and gave a real number,
but \textbf{a $1$-form will take both a point \emph{and} a tangent
vector at that point, and spit out a real number.}
So a $1$-form $\alpha$ is a smooth function on pairs $(p,v)$,
where $v$ is a tangent vector at $p$, to $\RR$.  Hence
\[ \alpha \colon U \times V \to \RR. \]

Actually, for any point $p$, we will require that $\alpha(p,-)$
is a linear function in terms of the vectors:
i.e.\ we want for example that $\alpha(p,2v) = 2\alpha(p,v)$.
So it is more customary to think of $\alpha$ as:
\begin{definition}
	A \vocab{$1$-form} $\alpha$ is a smooth function
	\[ \alpha \colon U \to V^\vee. \]
\end{definition}
Like with $Df$, we'll use $\alpha_p$ instead of $\alpha(p)$.
So, at every point $p$, $\alpha_p$ is some linear functional
that eats tangent vectors at $p$, and spits out a real number.
Thus, we think of $\alpha_p$ as an element of $V^\vee$;
\[ \alpha_p \in V^\vee. \]

\begin{remark}
	You might recall that, in high school calculus, the ``arc-length element'' $ds$ can be
	integrated along a curve: $\int_c ds$ is the length of the curve $c$.

	This is \emph{not} a $1$-form! More on this later.
\end{remark}

Next, we draw pictures of $2$-forms.
This should, for example, let us integrate over a blob
(a so-called $2$-cell) of the form
\[ c \colon [0,1] \times [0,1] \to U \]
i.e.\ for example, a square in $U$.
In the previous example with $1$-forms,
we looked at tangent vectors to the curve $c$.
This time, at points we will look at \emph{pairs} of tangent vectors
in $U$: in the same sense that lots of tangent vectors
approximate the entire curve, lots of tiny squares
will approximate the big square in $U$.
\begin{center}
	\begin{asy}
		bigblob("$U$");
		filldraw( (-2,-2)--(-2,2)--(2,2)--(2,-2)--cycle,
			opacity(0.4) + orange, red);
		label("$c$", (2,2), dir(45), red);
		for (real t = -1; t < 2; t += 1) {
			draw( (-2,t)--(2,t), red );
			draw( (t,-2)--(t,2), red );
		}
		pair p = (-1, -1);
		dot("$p$", p, dir(225), blue);
		pair v = p + dir(90);
		pair w = p + dir(0);
		draw(p--v, blue, EndArrow);
		draw(p--w, blue, EndArrow);
		label("$v$", v, dir(135), blue);
		label("$w$", w, dir(-45), blue);
	\end{asy}
\end{center}
So what should a $2$-form $\beta$ be?
As before, it should start by taking a point $p \in U$,
so $\beta_p$ is now a linear functional:
but this time, it should be a linear map on two vectors $v$ and $w$.
Here $v$ and $w$ are not tangent so much as their span cuts out
a small parallelogram. So, the right thing to do is in fact consider
\[ \beta_p \in V^\vee \wedge V^\vee. \]
That is, to use the wedge product to get a handle on
the idea that $v$ and $w$ span a parallelogram.
Another valid choice would have been $(V \wedge V)^\vee$;
in fact, the two are isomorphic,\footnote{We only consider finite-dimensional $V$.}
but it will be more convenient to write it in the former.\footnote{%
See \Cref{sec:wedge_product_dual}.}

\section{Pictures of exterior derivatives}
Next question:
\begin{moral}
	How can we build a $1$-form from a $0$-form?
\end{moral}
Let $f$ be a $0$-form on $U$; thus, we have a function $f \colon U \to \RR$.
Then in fact there is a very natural $1$-form on $U$ arising
from $f$, appropriately called $df$.
Namely, given a point $p$ and a tangent vector $v$,
the differential form $(df)_p$ returns the \emph{change in $f$ along $v$}.
In other words, it's just the total derivative $(Df)_p(v)$.
\begin{center}
	\begin{asy}
		bigblob("$U$");
		dot("$3$", (-2,1), red);
		dot("$\sqrt2$", (-1,-1), red);
		dot("$-1$", (2,2), red);
		dot("$0$", (-3,-3), red);
		draw((-1,-1)--(0,1), blue, EndArrow);
		label("$\sqrt2 + \varepsilon$", (0,1), dir(90), blue);
		label("$v$", (-0.5, 0), dir(0), blue);
	\end{asy}
\end{center}
Thus, $df$ measures ``the change in $f$''.

Now, even if I haven't defined integration yet,
given a curve $c$ from a point $a$ to $b$, what do you think
\[ \int_c df \]
should be equal to?
Remember that $df$ is the $1$-form that measures
``infinitesimal change in $f$''.
So if we add up all the change in $f$ along a path from $a$ to $b$,
then the answer we get should just be
\[ \int_c df = f(b) - f(a). \]
This is the first case of something we call Stokes' theorem.

Generalizing, how should we get from a $1$-form to a $2$-form?
At each point $p$, the $2$-form $\beta$ gives a $\beta_p$
which takes in a ``parallelogram'' and returns a real number.
Now suppose we have a $1$-form $\alpha$.
Then along each of the edges of a parallelogram,
with an appropriate sign convention the $1$-form $\alpha$ gives
us a real number.
So, given a $1$-form $\alpha$, we define $d\alpha$
to be the $2$-form that takes in a parallelogram
spanned by $v$ and $w$,
and returns \textbf{the measure of $\alpha$ along the boundary}.

Now, what happens if you integrate $d\alpha$ along the entire square $c$?
The right picture is that, if we think of each little square
as making up the big square, then the adjacent boundaries cancel out,
and all we are left is the main boundary.
This is again just a case of the so-called Stokes' theorem.

\begin{center}
	\begin{asy}
		bigblob("$U$");
		filldraw( (-2,-2)--(-2,2)--(2,2)--(2,-2)--cycle,
			opacity(0.4) + orange, red);
		label("$c$", (2,2), dir(45), red);
		for (real t = -1; t < 2; t += 1) {
			draw( (-2,t)--(2,t), red );
			draw( (t,-2)--(t,2), red );
		}
		pair p = (-1, -1);
		dot("$p$", p, dir(225), blue);
		pair v = p + dir(90);
		pair w = p + dir(0);
		pair x = w + v - p;
		draw(p--w, blue, EndArrow);
		draw(w--x, blue, EndArrow);
		draw(x--v, blue, EndArrow);
		draw(v--p, blue, EndArrow);
	\end{asy}
	\hspace{4em}
	\begin{minipage}[t]{6.2cm}
		\includegraphics[width=6cm]{media/stokes-patch.png}
		\\ \scriptsize Image from \cite{img:stokes}
	\end{minipage}
\end{center}


\section{Differential forms}
\prototype{Algebraically,
	something that looks like $f \ee_1^\vee \wedge \ee_2^\vee + \dots$,
	and geometrically, see the previous section.}

Let's now get a handle on what $dx$ means.
Fix a real vector space $V$ of dimension $n$,
and let $\ee_1$, \dots, $\ee_n$ be a standard basis.
Let $U$ be an open set.

\begin{definition}
	We define a \vocab{differential $k$-form} $\alpha$ on $U$
	to be a smooth (infinitely differentiable) map
	$\alpha \colon U \to \Lambda^k(V^\vee)$.
	(Here $\Lambda^k(V^\vee)$ is the wedge product.)
\end{definition}

Like with $Df$, we'll use $\alpha_p$ instead of $\alpha(p)$.

\begin{example}
	[$k$-forms for $k=0,1$]
	\listhack
	\begin{enumerate}[(a)]
		\item A $0$-form is just a function $U \to \RR$.
		\item A $1$-form is a function $U \to V^\vee$.
		For example,
		the total derivative $Df$ of a function $V \to \RR$ is a $1$-form.
		\item Let $V = \RR^3$ with standard basis $\ee_1$, $\ee_2$, $\ee_3$.
		Then a typical $2$-form is given by
		\[
			\alpha_p
			=
			f(p) \cdot \ee_1^\vee \wedge \ee_2^\vee
			+ g(p) \cdot \ee_1^\vee \wedge \ee_3^\vee
			+ h(p) \cdot \ee_2^\vee \wedge \ee_3^\vee
			\in \Lambda^2(V^\vee)
		\]
		where $f,g,h \colon V \to \RR$ are smooth functions.
	\end{enumerate}
\end{example}

Now, by the projection principle (\Cref{thm:project_principle}) we only have to specify
a function on each of $\binom nk$ basis elements of $\Lambda^k(V^\vee)$.
So, take any basis $\{\ee_i\}$ of $V$, and
take the usual basis for $\Lambda^k(V^\vee)$ of elements
\[ \ee_{i_1}^\vee \wedge \ee_{i_2}^\vee \wedge \dots \wedge \ee_{i_k}^\vee. \]
Thus, a general $k$-form takes the shape
\[ \alpha_p = \sum_{1 \le i_1 < \dots < i_k \le n}
	f_{i_1, \dots, i_k}(p) \cdot
	\ee_{i_1}^\vee \wedge \ee_{i_2}^\vee \wedge \dots \wedge \ee_{i_k}^\vee. \]
Since this is a huge nuisance to write, we will abbreviate this to just
\[ \alpha = \sum_I f_I \cdot de_I \]
where we understand the sum runs over $I = (i_1, \dots, i_k)$,
and $de_I$ represents $\ee_{i_1}^\vee \wedge \dots \wedge \ee_{i_k}^\vee$.

Now that we have an element $\Lambda^k(V^\vee)$, what can it do?
Well, first let me get the definition on the table, then tell you what it's doing.
\begin{definition}
	[How to evaluate a differential form at a point]
	\label{def:evaluate_differential_form}
	For linear functions $\xi_1, \dots, \xi_k \in V^\vee$
	and vectors $v_1, \dots, v_k \in V$, set
	\[
		(\xi_1 \wedge \dots \wedge \xi_k)(v_1, \dots, v_k)
		\defeq
		\det
		\begin{bmatrix}
			\xi_1(v_1) & \dots & \xi_1(v_k) \\
			\vdots & \ddots & \vdots \\
			\xi_k(v_1) & \dots & \xi_k(v_k)
		\end{bmatrix}.
	\]
	You can check that this is well-defined
	under e.g. $v \wedge w = -w \wedge v$ and so on.
\end{definition}

\begin{example}
	[Evaluation of a differential form]
	Set $V = \RR^3$.
	Suppose that at some point $p$, the $2$-form $\alpha$ returns
	\[ \alpha_p = 2 \ee_1^\vee \wedge \ee_2^\vee + \ee_1^\vee \wedge \ee_3^\vee. \]
	Let $v_1 = 3\ee_1 + \ee_2 + 4\ee_3$ and $v_2 = 8\ee_1 + 9\ee_2 + 5\ee_3$.
	Then
	\[
		\alpha_p(v_1, v_2)
		=
		2\det \begin{bmatrix}
			3 & 8 \\ 1 & 9 \end{bmatrix}
		+
		\det \begin{bmatrix}
			3 & 8 \\ 4 & 5 \end{bmatrix}
		= 21.
	\]
\end{example}

What does this definition mean?
One way to say it is that
\begin{moral}
	If I walk to a point $p \in U$,
	a $k$-form $\alpha$ will take in $k$ vectors $v_1, \dots, v_k$
	and spit out a number, which is to be interpreted as a (signed) volume.
\end{moral}

Picture:
\begin{center}
	\begin{asy}
		bigblob("$U$");
		pair p = (-2,-2);
		dot("$p$", p, dir(225), red);
		pair p1 = p + 1.4*dir(120);
		pair p2 = p + 1.7*dir(10);
		draw(p--p1, red, EndArrow);
		draw(p--p2, red, EndArrow);
		label("$v_1$", p1, dir(p1-p), red);
		label("$v_2$", p2, dir(p2-p), red);
		label("$\alpha_p(v_1, v_2) \in \mathbb R$", p+dir(45)*3);
	\end{asy}
\end{center}

In other words, at every point $p$, we get a function $\alpha_p$.
Then I can feed in $k$ vectors to $\alpha_p$ and get a number,
which I interpret as a signed volume of the parallelepiped spanned by the $\{v_i\}$'s
in some way (e.g.\ the flux of a force field).
That's why $\alpha_p$ as a ``function'' is contrived to lie in the wedge product:
this ensures that the notion of ``volume'' makes sense, so that for example,
the equality $\alpha_p(v_1, v_2) = -\alpha_p(v_2, v_1)$ holds.

This is what makes differential forms so fit for integration.

\section{Exterior derivatives}
\prototype{Possibly $(dx_1)_p = \ee_1^\vee$.}
We now define the exterior derivative\footnote{The name ``exterior derivative'' comes from the wedge
product $\wedge$ being alternatively called the exterior product.} $df$ that we gave
pictures of at the beginning of the chapter.
It turns out that the exterior derivative is easy to compute
given explicit coordinates to work with.

Firstly, we define the exterior derivative
of a function $f \colon U \to \RR$, as
\[ df \defeq Df = \sum_i \frac{\partial f}{\partial \ee_i} \ee_i^\vee \]
In particular, suppose $V = \RR^n$ and $f(x_1, \dots, x_n) = x_1$
(i.e.\ $f = \ee_1^\vee$). Then:
\begin{ques}
	Show that for any $p \in U$, \[ \left( d(\ee_1^\vee) \right)_p = \ee_1^\vee. \]
\end{ques}

\begin{abuse}
	Unfortunately, someone somewhere decided
	it would be a good idea to use ``$x_1$'' to denote $\ee_1^\vee$
	(because \emph{obviously}\footnote{Sarcasm.} $x_1$ means
	``the function that takes $(x_1, \dots, x_n) \in \RR^n$ to $x_1$'')
	and then decided that \[ dx_1 \defeq d(\ee_1^\vee). \]
	This notation is so entrenched that I have no choice
	but to grudgingly accept it.
	Note that it's not even right,
	since technically it's $(dx_1)_p = \ee_1^\vee$; $dx_1$ is a $1$-form.
	\label{abuse:dx}
\end{abuse}
\begin{remark}
	This is the reason why we use the notation $\frac{df}{dx}$ in calculus now:
	given, say, $f \colon \RR \to \RR$ by $f(x) = x^2$, it is indeed true that
	\[ df = 2x \cdot \ee_1^\vee = 2x \cdot dx \]
	and so by (more) abuse of notation we write $df/dx = 2x$.
\end{remark}
%TODO this is backwards, the actual reason is mathematician used that notation before the
%formalization using differential form (Newton etc.), and for convenience we update our definition
%to match the definition, and not vice versa.

More generally, we can define the exterior derivative
in terms of our basis $\ee_1$, \dots, $\ee_n$ as follows:
\begin{definition}
If $\alpha = \sum_I f_I de_I$ then we define the \vocab{exterior derivative} as
\[ d\alpha \defeq \sum_I df_I \wedge de_I
	= \sum_I \sum_j \fpartial{f_I}{\ee_j} d\ee_j \wedge de_I. \]
\end{definition}
This doesn't depend on the choice of basis.
(We'll mention a basis-free definition at the end of this section.)

\begin{example}[Computing some exterior derivatives]
	Let $V = \RR^3$ with standard basis $\ee_1$, $\ee_2$, $\ee_3$.
	Let $f(x,y,z) = x^4 + y^3 + 2xz$.
	Then we compute
	\[ df = Df = (4x^3+2z) \; dx + 3y^2 \; dy + 2x \; dz. \]
	Next, we can evaluate $d(df)$ as prescribed: it is
	\begin{align*}
		d^2f &= (12x^2 \; dx + 2 dz) \wedge dx + (6y \; dy) \wedge dy
		+ 2(dx \wedge dz) \\
		&= 12x^2 (dx \wedge dx) + 2(dz \wedge dx) + 6y (dy \wedge dy) + 2(dx \wedge dz) \\
		&= 2(dz \wedge dx) + 2(dx \wedge dz) \\
		&= 0.
	\end{align*}
	So surprisingly, $d^2f$ is the zero map.
	Here, we have exploited \Cref{abuse:dx} for the first time,
	in writing $dx$, $dy$, $dz$.
\end{example}
And in fact, this is always true in general:
\begin{theorem}[Exterior derivative vanishes]
	\label{thm:dd_zero}
	Let $\alpha$ be any $k$-form.
	Then $d^2(\alpha) = 0$.
	Even more succinctly, \[ d^2 = 0. \]
\end{theorem}
The proof is left as \Cref{prob:dd_zero}.
\begin{exercise}
	Compare the statement $d^2 = 0$ to the geometric
	picture of a $2$-form given at the beginning of this chapter.
	Why does this intuitively make sense?
\end{exercise}

Here are some other properties of $d$:
\begin{itemize}
	\ii As we just saw, $d^2 = 0$.
	\ii For a $k$-form $\alpha$ and $\ell$-form $\beta$, one can show that
	\[ d(\alpha \wedge \beta) = d\alpha \wedge \beta + (-1)^k (\alpha \wedge d\beta). \]
	\ii If $f \colon U \to \RR$ is smooth, then $df = Df$.
\end{itemize}
In fact, one can show that $df$ as defined above is
the \emph{unique} map sending $k$-forms to $(k+1)$-forms
with these properties.
So, one way to \emph{define} $df$ is to take as axioms
the bulleted properties above
and then declare $d$ to be the unique solution to this functional equation.
In any case, this tells us that our definition of $d$
does not depend on the basis chosen.

Recall that $df$ measures the change in boundary.
In that sense, $d^2 = 0$ is saying something like
``the boundary of the boundary is empty''.
We'll make this precise when we see Stokes' theorem in the next chapter.

\section{Digression: $\Lambda^k(V^\vee)$ versus $(\Lambda^k(V))^\vee$}
\label{sec:wedge_product_dual}

Earlier on, we remarked that $\Lambda^k(V^\vee) \cong (\Lambda^k(V))^\vee$ canonically, but we
use the former for convenience.

The former notation is indeed more convenient (wedge product of
two differential form is natural), but it's not clear why \Cref{def:evaluate_differential_form} is
defined in such a way.

If we used $(\Lambda^k(V))^\vee$ instead, it's trivial to evaluate a differential form:
For $f \in (\Lambda^k(V))^\vee$ and vectors $v_1, \dots, v_k \in V$, then
\[
	f(v_1, \dots, v_k) \defeq f(v_1 \wedge \dots \wedge v_k).
\]
This is because $f$ naturally takes in an element of $\Lambda^k(V)$ and returns a real number.

But now, it is not clear how we can take $f \in (\Lambda^1(V))^\vee$ and $g \in (\Lambda^1(V))$, and
return something like $f \wedge g \in (\Lambda^2(V))^\vee$: The natural choice of $(v \wedge w
\mapsto f(v) g(w)$ isn't even well-defined!\footnote{You can try it with $f = \ee_1^\vee$ and
$g = \ee_2^\vee$, evaluate it at $\ee_1 \wedge \ee_2$ and $-\ee_2 \wedge \ee_1$,
which we know is equal.}

To figure out what to do, we have to take a step back and consider the tensor product.
For a vector space $V$,
define $T^k(V) = \underbrace{V \otimes V \otimes \cdots \otimes V}_{\text{$k$ times}}$.

We have the following diagram:
\begin{center}
\begin{tikzcd}
	T^k(V^\vee) \ar[r, hook, "\phi"] \ar[d, surjective head, "q", shift left] & (T^k(V))^\vee \\
	\Lambda^k(V^\vee) \ar[u, hook, "\iota", shift left] & (\Lambda^k(V))^\vee \ar[u, hook, "q^\vee"]
\end{tikzcd}
\end{center}

What is going on?

First, there is a natural map $T^k(V^\vee) \to (T^k(V))^\vee$ given by
\[ \phi(\xi_1 \otimes \dots \otimes \xi_k) = v_1 \otimes \dots \otimes v_k \mapsto
\xi_1(v_1) \xi_2(v_2) \dotsm \xi_k(v_k) \]
and extends to all of $T^k(V^\vee)$ by linearity the obvious way.

Unlike the situation with the wedge product above, this map is indeed well-defined.

With some manual work, we can check $\phi$ is injective. Because both $T^k(V^\vee)$ and
$(T^k(V))^\vee$ has dimension $(\dim V)^k$, $\phi$ is bijective.

Next, note that $\Lambda^k(V)$ is just ``$T^k(V)$ but with more relations imposed'',
there is a natural quotient map $q \colon T^k(V) \surjto \Lambda^k(V)$.
So, the tensors are divided into equivalence classes.
\begin{example}
	If $V = \RR^2$, then $T^2(V)$ would have elements such as $\ee_1 \otimes \ee_1$, $\ee_1 \otimes
	\ee_2$ or $-\ee_2 \otimes \ee_1$.

	Mapping these elements to $\Lambda^2(V)$, we get $\ee_1 \wedge \ee_1 = 0$, and $\ee_1 \wedge
	\ee_2 = -\ee_2 \wedge \ee_1$, i.e. $\ee_1 \otimes \ee_2$ and $-\ee_2 \otimes \ee_1$ belongs to
	the same equivalence class.
\end{example}

The map $q$ induces a dual map $q^\vee \colon (\Lambda^k(V))^\vee \to (T^k(V))^\vee$.
\begin{ques}
	Convince yourself that a function $f \in (T^k(V))^\vee$ belongs to $\img q^\vee$ if and only if
	$f$ assigns the same value for every element in each equivalence class, as defined above.
\end{ques}

Thus, we get an isomorphism $q \circ \phi\inv \circ q^\vee \colon (\Lambda^k(V))^\vee \to
\Lambda^k(V^\vee)$.\footnote{We're using $q$ for both the map $T^k(V) \to \Lambda^k(V)$ and
$T^k(V^\vee) \to \Lambda^k(V^\vee)$, by abuse of notation.}

To check this is indeed an isomorphism, we will construct its inverse map.
As defined above, each equivalence class in $T^k(V^\vee)$ (fiber of $g \in \Lambda^k(V^\vee)$)
has multiple elements, but we can find a canonical element by the following:
\begin{definition}
	For vector space $V$, and element $f = f_1 \otimes f_2 \otimes \dots \otimes f_k \in T^k(V)$,
	we define the alternation of $f$ as follows:
	\[ \Alt f = \frac{1}{k!} \sum_{\sigma \in S_k} \opname{sgn}(\sigma) \cdot
	(f_{\sigma(1)} \otimes f_{\sigma(2)} \otimes \dots \otimes f_{\sigma(k)}) \]
	and extend it to all of $T^k(V)$.
\end{definition}
Here, $S_k$ is the permutation group. Notice the similarity with the definition of the determinant.

\begin{example}
	As above, $V = \RR^2$. Then we get:
	\[ \Alt (\ee_1 \otimes \ee_2) = \frac{\ee_1 \otimes \ee_2 - \ee_2 \otimes \ee_1}{2}. \]
	Notice that if we swap the first and second component of $\ee_1 \otimes \ee_2$, we get $\ee_2
	\otimes \ee_1$ which has little to do with the original tensor. However, if we swap
	the first and second component of $\frac{\ee_1 \otimes \ee_2 - \ee_2 \otimes \ee_1}{2}$, we get
	$\frac{\ee_2 \otimes \ee_1 - \ee_1 \otimes \ee_2}{2}$, which is exactly the negation of the
	original tensor!
\end{example}

We see that $\Alt f$ is a desirable representative of the equivalence class of $f$ because:
\begin{itemize}
	\ii $\Alt (\Alt f) = \Alt f$;
	\ii $q(f) = q(\Alt f)$ where $q$ is the quotient map $T^k(V) \surjto \Lambda^k(V)$;
	\ii $\Alt f$ is an \emph{alternating tensor} --- that is, if we swap two adjacent components
	of $\Alt f$ for each pure tensor, then the whole tensor gets negated.
\end{itemize}
Thus it makes sense for us to define $\iota \colon \Lambda^k(V^\vee) \injto T^k(V^\vee)$
that takes each element to the alternating tensor in $T^k(V^\vee)$.
\begin{example}
	With the same example as above, $V = \RR^2$, then we get
	\[
		\iota(\ee_1 \wedge \ee_2) = \Alt(\ee_1 \otimes \ee_2)
		= \frac{\ee_2 \otimes \ee_1 - \ee_1 \otimes \ee_2}{2}.
	\]
\end{example}
Finally,
\begin{exercise}
	Show that $\img (\phi \circ \iota) = \img q^\vee$, and that
	$\iota^\vee \circ \phi \circ \iota$ and $q \circ \phi\inv \circ q^\vee$
	are inverses of each other.
\end{exercise}

It is common notation that we want to define the wedge product such that
$\ee_1^\vee \wedge \ee_2^\vee$ takes in $\ee_1 \wedge \ee_2$ (that is, the square formed by $\ee_1$
and $\ee_2$), and returns $1$. However, if we define the wedge product naturally by the method
above, we get
\[
	\iota(\ee_1^\vee \wedge \ee_2^\vee)
	= \frac{\ee_1^\vee \otimes \ee_2^\vee - \ee_2^\vee \otimes \ee_1^\vee}{2}
\]
which means
\[
	\phi(\iota(\ee_1^\vee \wedge \ee_2^\vee))(\ee_1, \ee_2)
	= \frac{1 \cdot 1 - 0 \cdot 0}{2} = \frac{1}{2}.
\]
So, a corrective factor $k!$ is needed.

To see how ``difficult'' the wedge product will be if we use the second notation, let $V = \RR^3$,
$\alpha = dx \wedge dy \in \Lambda^2(V^\vee)$, and $\beta = dz \in \Lambda^1(V^\vee)$.

Then, we knows:
\begin{itemize}
	\ii $\alpha(\ee_1 \wedge \ee_2) = 1$.
	\ii $\beta(\ee_3) = 1$.
	\ii We should have $(\alpha \wedge \beta)(\ee_1 \wedge \ee_2 \wedge \ee_3) = 1$.
\end{itemize}
The last point is obvious if we let the wedge product be the map $\wedge \colon
\Lambda^2(V^\vee) \times \Lambda^1(V^\vee) \to \Lambda^3(V^\vee)$.

However, if we're given $\alpha$ and $\beta$ as elements of $(\Lambda^2(V))^\vee$ and
$(\Lambda^1(V))^\vee$ respectively (that is, we can only evaluate $\alpha$ at $v \wedge w$ for $v, w
\in V$; and we can only evaluate $\beta$ at $v$ for $v \in V$), then it would be much more difficult
to write down what $\alpha \wedge \beta$ should be. In fact,
\[
	(\alpha \wedge \beta)(v_1 \wedge v_2 \wedge v_3) =
	\alpha(v_1 \wedge v_2) \beta(v_3)
	- \alpha(v_1 \wedge v_3) \beta(v_2)
	+ \alpha(v_2 \wedge v_3) \beta(v_1).
\]
You can see that this is a variant of the alternation operator (or the evaluation operation),
where we compute a weighted average in order to force $\alpha \wedge \beta$ to be alternating.

\section{Tangential remark: $ds$ is not a $1$-form}

As mentioned in a remark earlier, $ds$ is not a
$1$-form.\footnote{\url{https://mathoverflow.net/q/90455} has a discussion on this.}

We said earlier that differential form is something you can integrate. You can certainly integrate
$ds$, but it's not considered a $1$-form!

What's going on here?

In fact, the true story is that the objects that are integrable over a smooth curve are
\vocab{$1$-densities}. We will define this later.

For simplicity, we work over $\RR^2$ in this section.
Given a (smooth) $1$-density $\omega$ that can be integrated over a smooth curve $c$,
we would like the integral $\int_c \omega$ to have the following properties:
\begin{itemize}
	\ii It is additive: if $c$ is the concatenation of two curves $c_1$ and $c_2$, then
	$\int_c \omega = \int_{c_1} \omega + \int_{c_2} \omega$.
	\ii Because everything is smooth, we would expect that if $c$ is a tiny line segment,
	then in fact $\int_{c_1} \omega \approx \int_{c_2} \omega$ if we divide $c$ into two segments
	$c_1$ and $c_2$ of equal length.

	Thus, it's natural to require $\int_c \omega$ to be ``approximately linear'' when the length of
	$c$ is small enough.

	In symbols: for $\eps > 0$, let $c_\eps$ be the initial segment of the curve $c$ with length
	$\eps$, then
	\[ \lim_{\eps \to 0^+} \frac{\int_{c_\eps} \omega}{\eps} = h \]
	for some finite constant $h$.
\end{itemize}

We certainly can formalize a $1$-density $\omega$ to be simply a function that takes smooth curves
$c$ and returns the value $\int_c \omega$ satisfying the two conditions above, but this definition
is clunky.

A better way to do it is to observe that, if we know $\int_c \omega$ for tiny curves $c$, then we
can integrate $\omega$ over any smooth curves $c$ by chopping it up into tiny curves.
But this isn't completely formal --- of course, as the length of a curve tends to $0$, the integral
$\int_c \omega$ also tends to 0 --- so instead, we consider the limit above:
\[ \lim_{\eps \to 0^+} \frac{\int_{c_\eps} \omega}{\eps}. \]

\begin{ques}
	Convince yourself that, given two curves $c \colon [0, 1] \to \RR^2$ and $c_2 \colon [0, 1] \to
	\RR^2$ that starts at the same point $c(0) = c_2(0) = p$,
	and moves in the same direction $c'(0) = c_2'(0) = v$, then basic smoothness condition of
	$\int_c \omega$ would guarantee that the limit above is the same.
\end{ques}

Thus,
\begin{moral}
We can define a $1$-density $\omega$ to be a function that takes in a point $p$ and the initial
direction $v \in \RR^2$, which is understood as a tangent vector of $\RR^2$ at $p$, and returns
the limit.
\end{moral}

Formally:
\begin{definition}[$1$-density]
	A $1$-density $\omega$ is a function $\RR^2 \times \RR^2 \to \RR$.
\end{definition}
We writes $\omega_p(v) \in \RR$.

Since only the direction matters, it makes sense to make $\omega$ satisfy
$\omega_p(\lambda v) = \lambda \omega_p(v)$ for $\lambda \geq 0$.  In particular, $\omega_p(0) = 0$.

Then, $ds$ is the differential form $ds_p(v) = \Vert v \Vert$. While we have not rigorously defined
how to integrate over a curve (we will do this next chapter), you can intuitively see how it works.

With this definition, a $1$-form is just a $1$-density that is in addition linear in the second
argument --- $\omega_p(v + w) = \omega_p(v) + \omega_p(w)$.

So, what is the special properties that differential forms enjoys? For one, if $\omega$ is a
differential form, we have:
\begin{quote}
	Let $c \colon [0, 1] \to \RR^2$ be a smooth curve, then for any sequence of smooth curves $c_k$
	that converges uniformly to $c$, then $\int_{c_k} \omega$ converges to $\int_c \omega$.
\end{quote}

You can easily imagine how this can fail for $ds$ --- a sequence of piecewise smooth curves
that consist of only horizontal and vertical lines can approximate a circle, but the arc length of
these jagged curves can never converge to the circumference of the circle.\footnote{In fact, using
the same argument, you can also prove that, conversely, any smooth density that satisfies the latter
property must in fact be linear!}

\section{Closed and exact forms}
Let $\alpha$ be a $k$-form.
\begin{definition}
	We say $\alpha$ is \vocab{closed} if $d\alpha = 0$.
\end{definition}
\begin{definition}
	We say $\alpha$ is \vocab{exact} if for some $(k-1)$-form $\beta$,
	$d\beta = \alpha$.  If $k = 0$, $\alpha$ is exact only when $\alpha = 0$.
\end{definition}
\begin{ques}
	Show that exact forms are closed.
\end{ques}

A natural question arises: are there closed forms
which are not exact?
Surprisingly, the answer to this question is tied to topology.
Here is one important example.

\begin{example}
	[The angle form]
	\label{ex:angle_form}
	Let $U = \RR^2 \setminus \{0\}$,
	and let $\theta(p)$ be the angle formed by the $x$-axis
	and the line from the origin to $p$.

	The $1$-form $\alpha \colon U \to (\RR^2)^\vee$ defined by
	\[ \alpha = \frac{-y \; dx + x \; dy}{x^2+y^2} \]
	is called the \vocab{angle form}:
	given $p \in U$ it measures the change in angle $\theta(p)$
	along a tangent vector.
	So intuitively, ``$\alpha = d\theta$''.
	Indeed, one can check directly that the angle form is closed.

	However, $\alpha$ is not exact: there is no global smooth
	function $\theta \colon U \to \RR$ having $\alpha$ as a derivative.
	This reflects the fact that one can actually perform
	a full $2\pi$ rotation around the origin, i.e.\ $\theta$
	only makes sense mod $2\pi$.
	Thus existence of the angle form $\alpha$ reflects
	the possibility of ``winding'' around the origin.
\end{example}

So the key idea is that the failure of a closed form to be exact
corresponds quite well with ``holes'' in the space:
the same information that homotopy and homology groups are trying to capture.
To draw another analogy, in complex analysis Cauchy-Goursat
only works when $U$ is simply connected.
The ``hole'' in $U$ is being detected by the existence of a form $\alpha$.
The so-called de Rham cohomology will make this relation explicit.

\section\problemhead
\begin{problem}
	Show directly that the angle form
	\[ \alpha = \frac{-y \; dx + x \; dy}{x^2+y^2} \]
	is closed.
\end{problem}

\begin{problem}
	\label{prob:dd_zero}
	Establish \Cref{thm:dd_zero}, which states that $d^2 = 0$.
	\begin{hint}
		This is just a summation.
		You will need the fact that mixed partials are symmetric.
	\end{hint}
\end{problem}
